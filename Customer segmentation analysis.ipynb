{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1707906-a4b7-4fa5-9f2e-400f63404f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import seaborn as sns\n",
    "import random \n",
    "import datetime as dt\n",
    "import re\n",
    "import pickle\n",
    "import nltk, warnings\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import digits, punctuation\n",
    "\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, Normalizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn import preprocessing, model_selection, metrics, feature_selection\n",
    "from sklearn.model_selection import GridSearchCV, learning_curve\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import neighbors, linear_model, svm, tree, ensemble\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d7063b-f2e6-451b-8403-630ec8c12f2c",
   "metadata": {},
   "source": [
    "Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c43297-094a-4dc2-9a77-51a48de94d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the database \n",
    "\n",
    "data = pd.read_csv(r"C:\Users\MANGLESHWARI\Downloads\customer segmentation\customer_segmentation.csv", encoding="latin1")
\n",
    "data.head(5)\n",
    "data.info()\n",
    "#Let's take a look at the missing values\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "data.isnull().mean(axis=0).plot.barh()\n",
    "plt.title(\"Ratio of missing values per columns\")\n",
    "#Let's go deeper in these missing values\n",
    "\n",
    "nan_rows = data[data.isnull().T.any().T]\n",
    "nan_rows.head(5)\n",
    "#Since the missing values are only in the CustomerID column and the description column, we could try to look at the InvoiceNo and see if maybe we can find the Customer ID\n",
    "\n",
    "data[data['InvoiceNo']== '536414']\n",
    "data[data['InvoiceNo']== '536544'][:5]\n",
    "#It appears that we can't replace the missing values and we can't keep data without the value in the customer id columns since we want to classify the customers. So I'll drop the lines with missing values on the customer ID column.\n",
    "\n",
    "data = data.dropna(subset=[\"CustomerID\"])\n",
    "plt.figure(figsize=(5, 5))\n",
    "data.isnull().mean(axis=0).plot.barh()\n",
    "plt.title(\"Ratio of missing values per columns\")\n",
    "#There are no more missing values. I'll now check the dupplicate values and drop them if there's any.\n",
    "\n",
    "print('Dupplicate entries: {}'.format(data.duplicated().sum()))\n",
    "data.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bc9a09-fd64-4bdc-afdc-37f2e063f32b",
   "metadata": {},
   "source": [
    "EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cc003c6-bc5c-4cc5-a59d-32afa3e81347",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m data.Country.nunique()\n\u001b[32m      2\u001b[39m customer_country=data[[\u001b[33m'\u001b[39m\u001b[33mCountry\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mCustomerID\u001b[39m\u001b[33m'\u001b[39m]].drop_duplicates()\n\u001b[32m      3\u001b[39m customer_country.groupby([\u001b[33m'\u001b[39m\u001b[33mCountry\u001b[39m\u001b[33m'\u001b[39m])[\u001b[33m'\u001b[39m\u001b[33mCustomerID\u001b[39m\u001b[33m'\u001b[39m].aggregate(\u001b[33m'\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m'\u001b[39m).reset_index().sort_values(\u001b[33m'\u001b[39m\u001b[33mCustomerID\u001b[39m\u001b[33m'\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "##Countries\n",
    "data.Country.nunique()\n",
    "customer_country=data[['Country','CustomerID']].drop_duplicates()\n",
    "customer_country.groupby(['Country'])['CustomerID'].aggregate('count').reset_index().sort_values('CustomerID', ascending=False)\n",
    "#More than 90% of the data is coming from UK !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7e9ccc-db9a-4567-a762-d022664feea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Quantity\n",
    "data.describe()\n",
    "data[(data['Quantity']<0)].head(5)\n",
    "#This is very interesting since we can see various things here :\n",
    "\n",
    "#The stock code values aren't only numerical, there are speciales values like D which means Discount\n",
    "#The InvoiceNo aren't also only numerical since there is a C before the other numbers for every negative value in the quantity column, this could mean that the order was canceled.\n",
    "#I'll analyse the InvoiceNo to find any patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea736ddd-dfcd-4c73-8c35-84bf806efd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##InvoiceNo - Cancelation Code\n",
    "# Constucting a basket for later use\n",
    "temp = data.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)['InvoiceDate'].count()\n",
    "nb_products_per_basket = temp.rename(columns = {'InvoiceDate':'Number of products'})\n",
    "nb_products_per_basket.InvoiceNo = nb_products_per_basket.InvoiceNo.astype(str)\n",
    "nb_products_per_basket['order_canceled'] = nb_products_per_basket['InvoiceNo'].apply(lambda x:int('C' in x))\n",
    "len(nb_products_per_basket[nb_products_per_basket['order_canceled']==1])/len(nb_products_per_basket)*100\n",
    "#It appears that more than 16% of the transactions were canceled which is significant. Let's take a look at some rows where the transaction was canceled.\n",
    "\n",
    "nb_products_per_basket[nb_products_per_basket['order_canceled']==1][:5]\n",
    "data[data['CustomerID'] == '12346']\n",
    "#By looking at these results, it appears that there is a counterpart to the canceled transaction in the database. Let's see if this is always the case.\n",
    "\n",
    "test = data[data['Quantity'] < 0][['CustomerID','Quantity',\n",
    "                                                   'StockCode','Description','UnitPrice']]\n",
    "for index, col in  test.iterrows():\n",
    "    if data[(data['CustomerID'] == col[0]) & (data['Quantity'] == -col[1]) \n",
    "                & (data['Description'] == col[2])].shape[0] == 0: \n",
    "        print(test.loc[index])\n",
    "        print('Our initial hypothesis is wrong')\n",
    "        break\n",
    "data[data['CustomerID'] == '14527'].head(5)\n",
    "#It appears that when there is a discount there are no counterparts. Let's try again but without the discount values\n",
    "\n",
    "data_check = data[(data['Quantity'] < 0) & (data['Description'] != 'Discount')][\n",
    "                                 ['CustomerID','Quantity','StockCode',\n",
    "                                  'Description','UnitPrice']]\n",
    "\n",
    "for index, col in  data_check.iterrows():\n",
    "    if data[(data['CustomerID'] == col[0]) & (data['Quantity'] == -col[1]) \n",
    "                & (data['Description'] == col[2])].shape[0] == 0: \n",
    "        print(index, data_check.loc[index])\n",
    "        print('The second hypothesis is also wrong')\n",
    "        break\n",
    "data[(data['CustomerID'] == '15311') & (data['Description'] == 'SET OF 3 COLOURED  FLYING DUCKS')]\n",
    "#It seems that the customer can also cancel just a part of the transaction which is logical so we need to take this into account for later.\n",
    "\n",
    "df_cleaned = data.copy(deep = True)\n",
    "df_cleaned['QuantityCanceled'] = 0\n",
    "\n",
    "entry_to_remove = [] ; doubtfull_entry = []\n",
    "\n",
    "for index, col in  data.iterrows():\n",
    "    if (col['Quantity'] > 0) or col['Description'] == 'Discount': continue        \n",
    "    df_test = data[(data['CustomerID'] == col['CustomerID']) &\n",
    "                         (data['StockCode']  == col['StockCode']) & \n",
    "                         (data['InvoiceDate'] < col['InvoiceDate']) & \n",
    "                         (data['Quantity']   > 0)].copy()\n",
    "    #_________________________________\n",
    "    # Cancelation WITHOUT counterpart\n",
    "    if (df_test.shape[0] == 0): \n",
    "        doubtfull_entry.append(index)\n",
    "    #________________________________\n",
    "    # Cancelation WITH a counterpart\n",
    "    elif (df_test.shape[0] == 1): \n",
    "        index_order = df_test.index[0]\n",
    "        df_cleaned.loc[index_order, 'QuantityCanceled'] = -col['Quantity']\n",
    "        entry_to_remove.append(index)        \n",
    "    #______________________________________________________________\n",
    "    # Various counterparts exist in orders: we delete the last one\n",
    "    elif (df_test.shape[0] > 1): \n",
    "        df_test.sort_index(axis=0 ,ascending=False, inplace = True)        \n",
    "        for ind, val in df_test.iterrows():\n",
    "            if val['Quantity'] < -col['Quantity']: continue\n",
    "            df_cleaned.loc[ind, 'QuantityCanceled'] = -col['Quantity']\n",
    "            entry_to_remove.append(index) \n",
    "            break  \n",
    "print(\"entry_to_remove: {}\".format(len(entry_to_remove)))\n",
    "print(\"doubtfull_entry: {}\".format(len(doubtfull_entry)))\n",
    "df_cleaned.drop(entry_to_remove, axis = 0, inplace = True)\n",
    "df_cleaned.drop(doubtfull_entry, axis = 0, inplace = True)\n",
    "remaining_entries = df_cleaned[(df_cleaned['Quantity'] < 0) & (df_cleaned['StockCode'] != 'D')]\n",
    "print(\"nb of entries to delete: {}\".format(remaining_entries.shape[0]))\n",
    "remaining_entries[:5]\n",
    "df_cleaned.drop(remaining_entries.index, axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a19671-7860-4258-87ba-254e7b4b4115",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Stockcode\n",
    "list_special_codes = df_cleaned[df_cleaned['StockCode'].str.contains('^[a-zA-Z]+', regex=True)]['StockCode'].unique()\n",
    "list_special_codes\n",
    "#These are specific operations which doesn't characterize our customers so I'll just drop these transactions from our database\n",
    "\n",
    "df_cleaned = df_cleaned[df_cleaned['StockCode']!= 'POST']\n",
    "df_cleaned = df_cleaned[df_cleaned['StockCode']!= 'D']\n",
    "df_cleaned = df_cleaned[df_cleaned['StockCode']!= 'C2']\n",
    "df_cleaned = df_cleaned[df_cleaned['StockCode']!= 'M']\n",
    "df_cleaned = df_cleaned[df_cleaned['StockCode']!= 'BANK CHARGES']\n",
    "df_cleaned = df_cleaned[df_cleaned['StockCode']!= 'PADS']\n",
    "df_cleaned = df_cleaned[df_cleaned['StockCode']!= 'DOT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb33936-1f86-4537-b0ed-047cf00b5aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Outliers\n",
    "df_cleaned.describe()\n",
    "#The minimum value for the unitprice is 0, let's see why is that.\n",
    "\n",
    "df_cleaned[(df_cleaned['UnitPrice'] == 0)].head(5)\n",
    "#I am tempted to replace the null values by the most common one but it might be a special discount or something else so I'll leave it like that. Here let's remove the items that got completely canceled in order to harmonize the futur clusters and not have too much special values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7af4f8-21f1-4976-9e20-aa5aa8e273f8",
   "metadata": {},
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73887111-8107-4f7d-bdfa-7deb8c3f5a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_counts(data):\n",
    "   for i in data.columns:\n",
    "       count = data[i].nunique()\n",
    "       print(i, \": \", count)\n",
    "unique_counts(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dde9873-927b-4ee0-8d06-a1e6343cf95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Total Price\n",
    "# Total price feature\n",
    "\n",
    "df_cleaned['TotalPrice'] = df_cleaned['UnitPrice'] * (df_cleaned['Quantity'] - df_cleaned['QuantityCanceled'])\n",
    "#We can now look at the countries' monetary value thanks to this feature\n",
    "\n",
    "revenue_per_countries = df_cleaned.groupby([\"Country\"])[\"TotalPrice\"].sum().sort_values()\n",
    "revenue_per_countries.plot(kind='barh', figsize=(15,12))\n",
    "plt.title(\"Revenue per Country\")\n",
    "No_invoice_per_country = df_cleaned.groupby([\"Country\"])[\"InvoiceNo\"].count().sort_values()\n",
    "No_invoice_per_country.plot(kind='barh', figsize=(15,12))\n",
    "plt.title(\"Number of Invoices per Country\")\n",
    "#This is very interesting since we can see that Netherlands is the 2nd country in value even though it has less invoices than countries like Germany or France for example and 10 times less customers. (95, 87 and 9 for Germany, France and Netherlands respectively)\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(df_cleaned['Country'])\n",
    "l = [i for i in range(37)]\n",
    "dict(zip(list(le.classes_), l))\n",
    "df_cleaned['Country'] = le.transform(df_cleaned['Country'])\n",
    "with open('labelencoder.pickle', 'wb') as g:\n",
    "    pickle.dump(le, g)\n",
    "df_cleaned.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f043651b-5f46-445e-8ddd-1e951708cc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "##RFM Principle\n",
    "#I'll implement here the RFM principle to classify the customers in this database. This part is inspired by the work of Susan Li. RFM stands for Recency, Frequency and Monetary. It is a customer segmentation technique that uses past purchase behavior to divide customers into groups.\n",
    "\n",
    "df_cleaned['InvoiceDate'].min()\n",
    "df_cleaned['InvoiceDate'].max()\n",
    "# I'll just fix the date to be one day after the last entry in the databse\n",
    "\n",
    "NOW = dt.datetime(2011,12,10)\n",
    "df_cleaned['InvoiceDate'] = pd.to_datetime(df_cleaned['InvoiceDate'])\n",
    "custom_aggregation = {}\n",
    "custom_aggregation[\"InvoiceDate\"] = lambda x:x.iloc[0]\n",
    "custom_aggregation[\"CustomerID\"] = lambda x:x.iloc[0]\n",
    "custom_aggregation[\"TotalPrice\"] = \"sum\"\n",
    "\n",
    "\n",
    "rfmTable = df_cleaned.groupby(\"InvoiceNo\").agg(custom_aggregation)\n",
    "rfmTable[\"Recency\"] = NOW - rfmTable[\"InvoiceDate\"]\n",
    "rfmTable[\"Recency\"] = pd.to_timedelta(rfmTable[\"Recency\"]).astype(\"timedelta64[D]\")\n",
    "rfmTable.head(5)\n",
    "custom_aggregation = {}\n",
    "\n",
    "custom_aggregation[\"Recency\"] = [\"min\", \"max\"]\n",
    "custom_aggregation[\"InvoiceDate\"] = lambda x: len(x)\n",
    "custom_aggregation[\"TotalPrice\"] = \"sum\"\n",
    "\n",
    "rfmTable_final = rfmTable.groupby(\"CustomerID\").agg(custom_aggregation)\n",
    "rfmTable_final.columns = [\"min_recency\", \"max_recency\", \"frequency\", \"monetary_value\"]\n",
    "rfmTable_final.head(5)\n",
    "first_customer = df_cleaned[df_cleaned['CustomerID']=='12747']\n",
    "first_customer.head(5)\n",
    "#The first customer has shopped a lot !\n",
    "\n",
    "quantiles = rfmTable_final.quantile(q=[0.25,0.5,0.75])\n",
    "quantiles = quantiles.to_dict()\n",
    "segmented_rfm = rfmTable_final\n",
    "def RScore(x,p,d):\n",
    "    if x <= d[p][0.25]:\n",
    "        return 1\n",
    "    elif x <= d[p][0.50]:\n",
    "        return 2\n",
    "    elif x <= d[p][0.75]: \n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "    \n",
    "def FMScore(x,p,d):\n",
    "    if x <= d[p][0.25]:\n",
    "        return 4\n",
    "    elif x <= d[p][0.50]:\n",
    "        return 3\n",
    "    elif x <= d[p][0.75]: \n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "#Here we'll apply a score on each feature of RFM\n",
    "\n",
    "segmented_rfm['r_quartile'] = segmented_rfm['min_recency'].apply(RScore, args=('min_recency',quantiles,))\n",
    "segmented_rfm['f_quartile'] = segmented_rfm['frequency'].apply(FMScore, args=('frequency',quantiles,))\n",
    "segmented_rfm['m_quartile'] = segmented_rfm['monetary_value'].apply(FMScore, args=('monetary_value',quantiles,))\n",
    "segmented_rfm.head()\n",
    "#Finally we'll set a score for each customer in the database.\n",
    "\n",
    "segmented_rfm['RFMScore'] = segmented_rfm.r_quartile.map(str) + segmented_rfm.f_quartile.map(str) + segmented_rfm.m_quartile.map(str)\n",
    "segmented_rfm.head()\n",
    "segmented_rfm[segmented_rfm['RFMScore']=='111'].sort_values('monetary_value', ascending=False)\n",
    "#Here we have an example of customers with a score of 111 which means that they are classified as our best customers.\n",
    "\n",
    "segmented_rfm.head(5)\n",
    "segmented_rfm = segmented_rfm.reset_index()\n",
    "segmented_rfm.head(5)\n",
    "df_cleaned = pd.merge(df_cleaned,segmented_rfm, on='CustomerID')\n",
    "df_cleaned.columns\n",
    "#We don't need the quartiles anymore, let's drop them.\n",
    "df_cleaned = df_cleaned.drop(columns=['r_quartile', 'f_quartile', 'm_quartile'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d172185-faa4-4f50-8f58-e352709caadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Time Features\n",
    "#I'll now create some time features, although I might not use them. It could be interesting to see if there are any paterns due to seasonality.\n",
    "\n",
    "df_cleaned['Month'] = df_cleaned[\"InvoiceDate\"].map(lambda x: x.month)\n",
    "df_cleaned['Month'].value_counts()\n",
    "df_cleaned['Weekday'] = df_cleaned[\"InvoiceDate\"].map(lambda x: x.weekday())\n",
    "df_cleaned['Day'] = df_cleaned[\"InvoiceDate\"].map(lambda x: x.day)\n",
    "df_cleaned['Hour'] = df_cleaned[\"InvoiceDate\"].map(lambda x: x.hour)\n",
    "df_cleaned.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2e4a6b-7ba8-4179-8476-675ea842e8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Product Categories\n",
    "X = df_cleaned[\"Description\"].unique()\n",
    "\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def stem_and_filter(doc):\n",
    "    tokens = [stemmer.stem(w) for w in analyzer(doc)]\n",
    "    return [token for token in tokens if token.isalpha()]\n",
    "\n",
    "analyzer = TfidfVectorizer().build_analyzer()\n",
    "CV = TfidfVectorizer(lowercase=True, stop_words=\"english\", analyzer=stem_and_filter, min_df=0.00, max_df=0.3)  # we remove words if it appears in more than 30 % of the corpus (not found stopwords like Box, Christmas and so on)\n",
    "TF_IDF_matrix = CV.fit_transform(X)\n",
    "print(\"TF_IDF_matrix :\", TF_IDF_matrix.shape, \"of\", TF_IDF_matrix.dtype)\n",
    "svd = TruncatedSVD(n_components = 100)\n",
    "normalizer = Normalizer(copy=False)\n",
    "\n",
    "TF_IDF_embedded = svd.fit_transform(TF_IDF_matrix)\n",
    "TF_IDF_embedded = normalizer.fit_transform(TF_IDF_embedded)\n",
    "print(\"TF_IDF_embedded :\", TF_IDF_embedded.shape, \"of\", TF_IDF_embedded.dtype)\n",
    "score_tfidf = []\n",
    "\n",
    "x = list(range(5, 155, 10))\n",
    "\n",
    "for n_clusters in x:\n",
    "    kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=10)\n",
    "    kmeans.fit(TF_IDF_embedded)\n",
    "    clusters = kmeans.predict(TF_IDF_embedded)\n",
    "    silhouette_avg = silhouette_score(TF_IDF_embedded, clusters)    rep = np.histogram(clusters, bins = n_clusters-1)[0]\n",
    "    score_tfidf.append(silhouette_avg)\n",
    "plt.figure(figsize=(20,16))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(x, score_tfidf, label=\"TF-IDF matrix\")\n",
    "plt.title(\"Evolution of the Silhouette Score\")\n",
    "plt.legend()\n",
    "The highest value for the silhouette score is when there are 135 clusters. So we'll chose this value.\n",
    "\n",
    "n_clusters = 135\n",
    "\n",
    "kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=30, random_state=0)\n",
    "proj = kmeans.fit_transform(TF_IDF_embedded)\n",
    "clusters = kmeans.predict(TF_IDF_embedded)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(proj[:,0], proj[:,1], c=clusters)\n",
    "plt.title(\"ACP with 135 clusters\", fontsize=\"20\")\n",
    "tsne = TSNE(n_components=2)\n",
    "proj = tsne.fit_transform(TF_IDF_embedded)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(proj[:,0], proj[:,1], c=clusters)\n",
    "plt.title(\"Visualization of the clustering with TSNE\", fontsize=\"20\")\n",
    "plt.figure(figsize=(20,8))\n",
    "wc = WordCloud()\n",
    "\n",
    "for num, cluster in enumerate(random.sample(range(100), 12)) :\n",
    "    plt.subplot(3, 4, num+1)\n",
    "    wc.generate(\" \".join(X[np.where(clusters==cluster)]))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.title(\"Cluster {}\".format(cluster))\n",
    "    plt.axis(\"off\")\n",
    "plt.figure()\n",
    "pd.Series(clusters).hist(bins=100)\n",
    "dict_article_to_cluster = {article : cluster for article, cluster in zip(X, clusters)}\n",
    "with open('product_clusters.pickle', 'wb') as h:\n",
    "    pickle.dump(dict_article_to_cluster, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b568d30f-009b-4d32-bb1b-a54d8865c5f8",
   "metadata": {},
   "source": [
    "Creating Customer Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6621977e-41ad-4308-bae0-9566be8c1645",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intermediate dataset grouped by invoices\n",
    "cluster = df_cleaned['Description'].apply(lambda x : dict_article_to_cluster[x])\n",
    "df2 = pd.get_dummies(cluster, prefix=\"Cluster\").mul(df_cleaned[\"TotalPrice\"], 0)\n",
    "df2 = pd.concat([df_cleaned['InvoiceNo'], df2], axis=1)\n",
    "df2_grouped = df2.groupby('InvoiceNo').sum()\n",
    "custom_aggregation = {}\n",
    "custom_aggregation[\"TotalPrice\"] = lambda x:x.iloc[0]\n",
    "custom_aggregation[\"min_recency\"] = lambda x:x.iloc[0]\n",
    "custom_aggregation[\"max_recency\"] = lambda x:x.iloc[0]\n",
    "custom_aggregation[\"frequency\"] = lambda x:x.iloc[0]\n",
    "custom_aggregation[\"monetary_value\"] = lambda x:x.iloc[0]\n",
    "custom_aggregation[\"CustomerID\"] = lambda x:x.iloc[0]\n",
    "custom_aggregation[\"Quantity\"] = \"sum\"\n",
    "custom_aggregation[\"Country\"] = lambda x:x.iloc[0]\n",
    "\n",
    "\n",
    "df_grouped = df_cleaned.groupby(\"InvoiceNo\").agg(custom_aggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e3cf24-d0d3-4db6-9693-d7e786573587",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final dataset grouped by customers\n",
    "df2_grouped_final = pd.concat([df_grouped['CustomerID'], df2_grouped], axis=1).set_index(\"CustomerID\").groupby(\"CustomerID\").sum()\n",
    "df2_grouped_final = df2_grouped_final.div(df2_grouped_final.sum(axis=1), axis=0)\n",
    "df2_grouped_final = df2_grouped_final.fillna(0)\n",
    "custom_aggregation = {}\n",
    "custom_aggregation[\"TotalPrice\"] = ['min','max','mean']\n",
    "custom_aggregation[\"min_recency\"] = lambda x:x.iloc[0]\n",
    "custom_aggregation[\"max_recency\"] = lambda x:x.iloc[0]\n",
    "custom_aggregation[\"frequency\"] = lambda x:x.iloc[0]\n",
    "custom_aggregation[\"monetary_value\"] = lambda x:x.iloc[0]\n",
    "custom_aggregation[\"Quantity\"] = \"sum\"\n",
    "custom_aggregation[\"Country\"] = lambda x:x.iloc[0]\n",
    "\n",
    "df_grouped_final = df_grouped.groupby(\"CustomerID\").agg(custom_aggregation)\n",
    "df_grouped_final.head(5)\n",
    "df_grouped_final.columns = [\"min\", \"max\", \"mean\", \"min_recency\", \"max_recency\", \"frequency\", \"monetary_value\", \"quantity\", \"country\"]\n",
    "df_grouped_final.head(5)\n",
    "df2_grouped_final.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf0e00d-7b07-4224-accb-74eabf838b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering Customers\n",
    "X1 = df_grouped_final.as_matrix()\n",
    "X2 = df2_grouped_final.as_matrix()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X1 = scaler.fit_transform(X1)\n",
    "X_final_std_scale = np.concatenate((X1, X2), axis=1)\n",
    "x = list(range(2, 12))\n",
    "y_std = []\n",
    "for n_clusters in x:\n",
    "    print(\"n_clusters =\", n_clusters)\n",
    "    \n",
    "    kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=10)\n",
    "    kmeans.fit(X_final_std_scale)\n",
    "    clusters = kmeans.predict(X_final_std_scale)\n",
    "    silhouette_avg = silhouette_score(X_final_std_scale, clusters)\n",
    "    y_std.append(silhouette_avg)\n",
    "    print(\"The average silhouette_score is :\", silhouette_avg, \"with Std Scaling\")\n",
    "#We want to have at least 5, 6 clusters so we won't take 2 or 3 clusters even though they have the highest silhouette scores, 8 clusters would fit the best here.\n",
    "kmeans = KMeans(init='k-means++', n_clusters = 8, n_init=30, random_state=0)  # random state just to be able to provide cluster number durint analysis\n",
    "kmeans.fit(X_final_std_scale)\n",
    "clusters = kmeans.predict(X_final_std_scale)\n",
    "plt.figure(figsize = (20,8))\n",
    "n, bins, patches = plt.hist(clusters, bins=8)\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.title(\"Number of customers per cluster\")\n",
    "plt.xticks([rect.get_x()+ rect.get_width() / 2 for rect in patches], [\"Cluster {}\".format(x) for x in range(8)])\n",
    "\n",
    "for rect in patches:\n",
    "    y_value = rect.get_height()\n",
    "    x_value = rect.get_x() + rect.get_width() / 2\n",
    "\n",
    "    space = 5\n",
    "    va = 'bottom'\n",
    "    label = str(int(y_value))\n",
    "    \n",
    "    plt.annotate(\n",
    "        label,                      \n",
    "        (x_value, y_value),         \n",
    "        xytext=(0, space),          \n",
    "        textcoords=\"offset points\", \n",
    "        ha='center',                \n",
    "        va=va)\n",
    "df_grouped_final[\"cluster\"] = clusters\n",
    "final_dataset = pd.concat([df_grouped_final, df2_grouped_final], axis = 1)\n",
    "final_dataset.head()\n",
    "final_dataset_V2 = final_dataset.reset_index()\n",
    "final_dataset_V2.to_csv(\"final_dataset_V2.csv\",index=False)\n",
    "with open('df_cleaned.pickle', 'wb') as f:\n",
    "    pickle.dump(df_cleaned, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95596b87-815c-433c-9981-219fbed51528",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interpreting the clusters\n",
    "tsne = TSNE(n_components=2)\n",
    "proj = tsne.fit_transform(X_final_std_scale)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(proj[:,0], proj[:,1], c=clusters)\n",
    "plt.title(\"Visualization of the clustering with TSNE\", fontsize=\"25\")\n",
    "#Graphically the clusters are distinctive enough. Let's take a closer look at the clusters that contain few customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2128f0a9-e595-4741-820a-71d8a0cd6520",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Cluster 0\n",
    "final_dataset[final_dataset['cluster']==0]\n",
    "final_dataset[final_dataset['cluster']==0].mean()\n",
    "temp_final_df = final_dataset.reset_index()\n",
    "cust0 = list(temp_final_df[temp_final_df['cluster']==0]['CustomerID'])\n",
    "cluster0 = df_cleaned[df_cleaned['CustomerID'].isin(cust0)]\n",
    "cluster0[['Quantity', 'UnitPrice', 'QuantityCanceled', 'TotalPrice', 'frequency', 'min_recency'\n",
    "         , 'monetary_value']].mean()\n",
    "cluster0['Description'].value_counts()[:10]\n",
    "custom_aggregation = {}\n",
    "custom_aggregation[\"Country\"] = lambda x:x.iloc[0]\n",
    "custom_aggregation[\"RFMScore\"] = lambda x:x.iloc[0]\n",
    "\n",
    "cluster0_grouped = cluster0.groupby(\"CustomerID\").agg(custom_aggregation)\n",
    "cluster0_grouped['RFMScore'].value_counts()\n",
    "cluster0_grouped['Country'].value_counts()\n",
    "cluster0['Month'].value_counts()\n",
    "plt.figure(figsize = (20,8))\n",
    "n, bins, patches = plt.hist(cluster0['Month'], bins=12)\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.title(\"Number of invoices per month\")\n",
    "plt.xticks([rect.get_x()+ rect.get_width() / 2 for rect in patches], [\"Month {}\".format(x) for x in range(1, 13)])\n",
    "\n",
    "for rect in patches:\n",
    "    y_value = rect.get_height()\n",
    "    x_value = rect.get_x() + rect.get_width() / 2\n",
    "\n",
    "    space = 5\n",
    "    va = 'bottom'\n",
    "    label = str(int(y_value))\n",
    "    \n",
    "    plt.annotate(\n",
    "        label,                      \n",
    "        (x_value, y_value),         \n",
    "        xytext=(0, space),          \n",
    "        textcoords=\"offset points\", \n",
    "        ha='center',                \n",
    "        va=va)\n",
    "temp['Year'] = cluster0[cluster0['Month']==12]['InvoiceDate'].map(lambda x: x.year)\n",
    "temp['Year'].value_counts()\n",
    "plt.figure(figsize = (20,8))\n",
    "n, bins, patches = plt.hist(cluster0['Weekday'], bins=7)\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.title(\"Number of invoices per day of the week\")\n",
    "plt.xticks([rect.get_x()+ rect.get_width() / 2 for rect in patches], [\"Day {}\".format(x) for x in range(0, 7)])\n",
    "\n",
    "for rect in patches:\n",
    "    y_value = rect.get_height()\n",
    "    x_value = rect.get_x() + rect.get_width() / 2\n",
    "\n",
    "    space = 5\n",
    "    va = 'bottom'\n",
    "    label = str(int(y_value))\n",
    "    \n",
    "    plt.annotate(\n",
    "        label,                      \n",
    "        (x_value, y_value),         \n",
    "        xytext=(0, space),          \n",
    "        textcoords=\"offset points\", \n",
    "        ha='center',                \n",
    "        va=va)\n",
    "cluster0['Day'].nunique()\n",
    "plt.figure(figsize = (20,8))\n",
    "n, bins, patches = plt.hist(cluster0['Day'], bins=31)\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.title(\"Number of invoices per day of the month\")\n",
    "plt.xticks([rect.get_x()+ rect.get_width() / 2 for rect in patches], [\"Day {}\".format(x) for x in range(1,32)])\n",
    "\n",
    "for rect in patches:\n",
    "    y_value = rect.get_height()\n",
    "    x_value = rect.get_x() + rect.get_width() / 2\n",
    "\n",
    "    space = 5\n",
    "    va = 'bottom'\n",
    "    label = str(int(y_value))\n",
    "    \n",
    "    plt.annotate(\n",
    "        label,                      \n",
    "        (x_value, y_value),         \n",
    "        xytext=(0, space),          \n",
    "        textcoords=\"offset points\", \n",
    "        ha='center',                \n",
    "        va=va)\n",
    "cluster0['Hour'].nunique()\n",
    "plt.figure(figsize = (20,8))\n",
    "n, bins, patches = plt.hist(cluster0['Hour'], bins=14)\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.title(\"Number of invoices per hour of the day\")\n",
    "plt.xticks([rect.get_x()+ rect.get_width() / 2 for rect in patches], [\"Hour {}\".format(x) for x in (sorted(cluster0['Hour'].unique()))])\n",
    "\n",
    "for rect in patches:\n",
    "    y_value = rect.get_height()\n",
    "    x_value = rect.get_x() + rect.get_width() / 2\n",
    "\n",
    "    space = 5\n",
    "    va = 'bottom'\n",
    "    label = str(int(y_value))\n",
    "    \n",
    "    plt.annotate(\n",
    "        label,                      \n",
    "        (x_value, y_value),         \n",
    "        xytext=(0, space),          \n",
    "        textcoords=\"offset points\", \n",
    "        ha='center',                \n",
    "        va=va)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faa7f9c-34e3-4191-a82a-21223cd3b900",
   "metadata": {},
   "source": [
    "These customers seems to be good for datazone since they have good RFM scores, the 4 most represented categories are (111, 211, 322, 222). They seem to be normal customers.\n",
    "\n",
    "Key figures:\n",
    "\n",
    "Min Basket Price: 10.86\n",
    "\n",
    "Mean Basket Price: 30.60\n",
    "\n",
    "Max Basket Price: 68.57\n",
    "\n",
    "Quantity: 10.00\n",
    "\n",
    "UnitPrice: 2.87\n",
    "\n",
    "QuantityCanceled: 0.04\n",
    "\n",
    "TotalPrice: 17.09\n",
    "\n",
    "Frequency 11.134050\n",
    "\n",
    "Recency 24.574626\n",
    "\n",
    "TOP 10 bought products :\n",
    "\n",
    "WHITE HANGING HEART T-LIGHT HOLDER: 1345\n",
    "JUMBO BAG RED RETROSPOT: 1079\n",
    "REGENCY CAKESTAND 3 TIER: 960\n",
    "ASSORTED COLOUR BIRD ORNAMENT: 926\n",
    "PARTY BUNTING: 924\n",
    "LUNCH BAG RED RETROSPOT: 898\n",
    "LUNCH BAG BLACK SKULL: 753\n",
    "SET OF 3 CAKE TINS PANTRY DESIGN: 725\n",
    "LUNCH BAG CARS BLUE: 679\n",
    "LUNCH BAG PINK POLKADOT: 676\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffc5647-bb98-4354-8a38-fa82e4e04a67",
   "metadata": {},
   "source": [
    " Cluster 1\n",
    "\n",
    "I already have done all the same analysis for the other clusters, so I won't put all of the code here but the idea is the same\n",
    "\n",
    "This cluster represents almost lost customers. The weird part about them is that there are some months when they didn't shop at all, it looks like a pattern.\n",
    "\n",
    "Key figures:\n",
    "\n",
    "Min Basket Price: 20.83\n",
    "\n",
    "Mean Basket Price: 33.77\n",
    "\n",
    "Max Basket Price: 26.43\n",
    "\n",
    "Quantity: 9.06\n",
    "\n",
    "UnitPrice: 2.68\n",
    "\n",
    "QuantityCanceled: 0.02\n",
    "\n",
    "TotalPrice: 13.77\n",
    "\n",
    "Frequency 3.065758\n",
    "\n",
    "Recency 36.131902\n",
    "\n",
    "TOP 10 bought products :\n",
    "\n",
    "PAPER CHAIN KIT 50'S CHRISTMAS: 267\n",
    "BAKING SET 9 PIECE RETROSPOT: 263\n",
    "WHITE HANGING HEART T-LIGHT HOLDER: 250\n",
    "ASSORTED COLOUR BIRD ORNAMENT: 247\n",
    "REX CASH+CARRY JUMBO SHOPPER: 223\n",
    "HOT WATER BOTTLE KEEP CALM: 215\n",
    "REGENCY CAKESTAND 3 TIER: 208\n",
    "RABBIT NIGHT LIGHT: 200\n",
    "GARDENERS KNEELING PAD KEEP CALM: 194\n",
    "SPOTTY BUNTING: 193\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1303464a-d0d0-4105-845c-d54477fa814d",
   "metadata": {},
   "source": [
    "Cluster 2\n",
    "\n",
    "The cluster 2 represents the best customers with a high recency which have around 60 visits, a lot of quantity bought on average, a high moneraty value and also a high frequency around 60 visits. These customers must be taken care.\n",
    "\n",
    "Min Basket Price : 13\n",
    "Mean Basket Price : 513\n",
    "Max Basket Price : 3812\n",
    "Quantity 117.083422\n",
    "UnitPrice 2.830180\n",
    "QuantityCanceled 0.069282\n",
    "TotalPrice 258.683970\n",
    "frequency 67.812655\n",
    "min_recency 1.679039"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32d8353-24cd-4531-ac71-c76c37bd1146",
   "metadata": {},
   "source": [
    " Cluster 3\n",
    "\n",
    "This cluster is full of lost customers. Indeed, as we can see in the month histogramm there are almost no invoices after july. We can see that there are in december but it's december of the past year. So this cluster is pretty bad for datazone, they don't want to have new customers in there. Furthermore they are cheap customers since the mean basket price is 28.91$ ...\n",
    "\n",
    "Key figures:\n",
    "\n",
    "Min Basket Price: 24.20\n",
    "\n",
    "Mean Basket Price: 28.91\n",
    "\n",
    "Max Basket Price: 34.52\n",
    "\n",
    "Quantity: 8.25\n",
    "\n",
    "UnitPrice: 3.29\n",
    "\n",
    "QuantityCanceled: 0.04\n",
    "\n",
    "TotalPrice: 15.20\n",
    "\n",
    "Frequency 2.606359\n",
    "\n",
    "Recency 237.013433\n",
    "\n",
    "TOP 10 bought products :\n",
    "\n",
    "WHITE HANGING HEART T-LIGHT HOLDER: 227\n",
    "REGENCY CAKESTAND 3 TIER: 182\n",
    "PARTY BUNTING: 137\n",
    "ASSORTED COLOUR BIRD ORNAMENT: 125\n",
    "REX CASH+CARRY JUMBO SHOPPER: 103\n",
    "SET OF 3 CAKE TINS PANTRY DESIGN: 100\n",
    "NATURAL SLATE HEART CHALKBOARD: 100\n",
    "JAM MAKING SET WITH JARS: 99\n",
    "HEART OF WICKER SMALL: 98\n",
    "HEART OF WICKER LARGE: 86"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542d5f7f-4aa9-4e01-b961-57da3d028b65",
   "metadata": {},
   "source": [
    "Cluster 4\n",
    "\n",
    "This cluster is quiete heterogeneous since there are 17 best customers, 6 lost cheap customers and so on. They do have a high mean basket price of 505 but it's mostly due to the mean quantity they buy (130) because the mean unit price is very low (3.26)\n",
    "\n",
    "For the time features, what is interesting is that these customers shop less on weekend and they shopped more at the end of the year.\n",
    "\n",
    "Min Basket Price : 247\n",
    "Mean Basket Price : 505\n",
    "Max Basket Price : 1023\n",
    "Quantity 130.299145\n",
    "UnitPrice 3.264359\n",
    "QuantityCanceled 2.332590\n",
    "TotalPrice 184.308595\n",
    "TOP 10 products bought :\n",
    "\n",
    "JUMBO BAG RED RETROSPOT : 38\n",
    "BLACK RECORD COVER FRAME : 31\n",
    "RECORD FRAME 7\" SINGLE SIZE : 28\n",
    "REGENCY CAKESTAND 3 TIER : 25\n",
    "WORLD WAR 2 GLIDERS ASSTD DESIGNS : 24\n",
    "WHITE HANGING HEART T-LIGHT HOLDER : 24\n",
    "PARTY BUNTING : 24\n",
    "LUNCH BOX I LOVE LONDON : 23\n",
    "RED HARMONICA IN BOX : 23\n",
    "CHILLI LIGHTS : 23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b57bc8-4c80-4409-be2f-5d82f07792dd",
   "metadata": {},
   "source": [
    "Cluster 5\n",
    "\n",
    "The cluster 5 contains 3 customers which are very much alike. Indeed, they bought only once or twice a few items at a huge quantity. It might be some profesionnals which bought it at discount and will sell back the commodity. Even if they have a high monetary value they're not very interesting for Datazon and we could consider them as lost customers.\n",
    "\n",
    "Min Basket Price : 3368\n",
    "Mean Basket Price : 3697\n",
    "Max Basket Price : 3533\n",
    "Quantity 2213.777778\n",
    "UnitPrice 2.386667\n",
    "QuantityCanceled 0.000000\n",
    "TotalPrice 3890.091111\n",
    "Frequency 1.666667\n",
    "Min_recency 210.888889"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa014e0-5b53-4a97-af85-878cf8589a21",
   "metadata": {},
   "source": [
    "Cluster 6\n",
    "\n",
    "What is very specific about this cluster is that there are no customers from UK, it's only foreign countries (Germany, France, Belgium, Italy and Finland). This cluster is also heterogeneous in terms of RFM since the 2 most represented categories are Best customer and Lost cheap customer. The average basket is very low (33) comparing the ones above but I guess that the more customers we have in a cluster and the more the average customer will be represented which doesn't spent 500$ per transactions like the ones above.\n",
    "\n",
    "October and november have the most invoices which isn't surpring approaching Christmas.\n",
    "\n",
    "Key figures:\n",
    "\n",
    "Min Basket Price: 20.58\n",
    "\n",
    "Mean Basket Price: 33.55\n",
    "\n",
    "Max Basket Price: 59.31\n",
    "\n",
    "Quantity: 13.785663\n",
    "\n",
    "UnitPrice: 2.884687\n",
    "\n",
    "QuantityCanceled: 0.057975\n",
    "\n",
    "TotalPrice: 23.749951\n",
    "\n",
    "Frequency 7.865563\n",
    "\n",
    "Recency 46.622343\n",
    "\n",
    "TOP 10 bought products :\n",
    "ROUND SNACK BOXES SET OF4 WOODLAND: 233\n",
    "REGENCY CAKESTAND 3 TIER: 161\n",
    "PLASTERS IN TIN WOODLAND ANIMALS: 150\n",
    "ROUND SNACK BOXES SET OF 4 FRUITS: 146\n",
    "RED TOADSTOOL LED NIGHT LIGHT: 144\n",
    "PLASTERS IN TIN CIRCUS PARADE: 141\n",
    "SPACEBOY LUNCH BOX: 137\n",
    "RABBIT NIGHT LIGHT: 120\n",
    "PLASTERS IN TIN SPACEBOY: 120\n",
    "WOODLAND CHARLOTTE BAG: 111"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aee1541-7b4d-4b24-9229-687db6eb9cf6",
   "metadata": {},
   "source": [
    " Cluster 7\n",
    "\n",
    "The cluster 7 contains 19 customers who are considered as best customers since they by the most, very frequently (75) and recently. The difference with cluster 2 is that they cluster 7's customers buy more frequently (75 vs 60) but have a lower monetary value (58000 vs 249000). They have a mean basket price lower than the other clusters.\n",
    "\n",
    "Min Basket Price : 10\n",
    "Mean Basket Price : 138\n",
    "Max Basket Price : 648\n",
    "Quantity 23.257769\n",
    "UnitPrice 2.615444\n",
    "QuantityCanceled 0.109129\n",
    "TotalPrice 34.916436\n",
    "Frequency 121.570291\n",
    "Recency 2.599109\n",
    "TOP 10 products bought :\n",
    "\n",
    "REGENCY CAKESTAND 3 TIER: 136\n",
    "JUMBO BAG RED RETROSPOT: 135\n",
    "WHITE HANGING HEART T-LIGHT HOLDER: 121\n",
    "CHILLI LIGHTS: 102\n",
    "PAPER BUNTING RETROSPOT: 97\n",
    "LUNCH BAG BLACK SKULL: 95\n",
    "GUMBALL COAT RACK: 93\n",
    "LUNCH BAG RED RETROSPOT: 91\n",
    "JUMBO BAG PINK POLKADOT: 84\n",
    "LUNCH BAG CARS BLUE: 81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d8b1d8-74d7-4e5e-9ff5-ce810ce7d3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Conclusion\n",
    "\n",
    "Let's quickly classify the clusters in terms of importance to Datazon :\n",
    "\n",
    "Cluster 2: high frequency with a lot of quantity (mean basket price of 513) bought on average and high monetary value (VIP clients)\n",
    "Cluster 7 : very high purchase frequency with a mean basket price of 150 but good monetary value.\n",
    "Cluster 4: very high basket price (huge quantity of products bought on average)\n",
    "Cluster 0: good average customers\n",
    "Cluster 6: good foreign customers\n",
    "Cluster 1: almost lost customers\n",
    "Cluster 5: highest monetary value but only one or two purchases over the year\n",
    "Cluster 3: lost customers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
